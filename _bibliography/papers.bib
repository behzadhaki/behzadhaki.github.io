---
---

@string{aps = {American Physical Society,}}


@inproceedings{haki_behzad_2022_7088343,
  bibtex_show={true},
  author       = {Haki, Behzad and
                  Nieto, Marina and
                  Pelinski, Teresa and
                  Jordà, Sergi},
  title        = {{Real-Time Drum Accompaniment Using Transformer
                   Architecture}},
  booktitle    = {{Proceedings of the 3rd Conference on AI Music
                   Creativity}},
  year         = {2022},
  abstract     = {This paper presents a real-time drum generation system capable of accompanying a human instrumentalist. The drum generation model is a transformer encoder trained to predict a short drum pattern given a reduced rhythmic representation. We demonstrate that with certain design considerations, the short drum pattern generator can be used as a real-time accompaniment in musical sessions lasting much longer than the duration of the training samples. A discussion on the potentials, limitations and possible future continuations of this work is provided.},
  publisher    = {AIMC},
  month        = sep,
  doi          = {10.5281/zenodo.7088343},
  url          = {https://doi.org/10.5281/zenodo.7088343},
  pdf = {Haki_2022__Real-Time_Drum_Accompaniment_Using_Transformer_Architecture.pdf},
  slides = {https://youtu.be/XOTksswM_4U},
  code = {https://github.com/behzadhaki/MonotonicGrooveTransformer},
  preview={RealTime_Drum_Accompaniment_Using_Trans_cropped.jpg}
}


@inproceedings{NIME21_33,
  bibtex_show={true},
  article-number = {33},
  author = {Nuttall, Thomas and Haki, Behzad and Jorda, Sergi},
  title = {Transformer Neural Networks for Automated Rhythm Generation},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.fe9a0d82},
  url = {https://nime.pubpub.org/pub/8947fhly},
  abstract = {Recent applications of Transformer neural networks in the field of music have demonstrated their ability to effectively capture and emulate long-term dependencies characteristic of human notions of musicality and creative merit. We propose a novel approach to automated symbolic rhythm generation, where a Transformer-XL model trained on the Magenta Groove MIDI Dataset is used for the tasks of sequence generation and continuation. Hundreds of generations are evaluated using blind-listening tests to determine the extent to which the aspects of rhythm we understand to be valuable are learnt and reproduced. Our model is able to achieve a standard of rhythmic production comparable to human playing across arbitrarily long time periods and multiple playing styles.},
  presentation-video = {https://youtu.be/Ul9s8qSMUgU},
  pdf = {https://nime.pubpub.org/pub/8947fhly/release/1},
  slides = {https://youtu.be/Ul9s8qSMUgU},
  code = {https://github.com/thomasgnuttall/bumblebeat},
  website = {http://nime2021.org/program/#/paper/100},
  preview={transformerXlDrumThomas.jpg}

}

@inproceedings{haki2019,
  bibtex_show={true},
  author = {haki, behzad and Jorda, Sergi},
  title = {A Bassline Generation System Based on Sequence-to-Sequence Learning},
  pages = {204--209},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Queiroz, Marcelo and Sedó, Anna Xambó},
  year = {2019},
  month = jun,
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672928},
  abstract     = {This paper presents a detailed explanation of a system generating basslines that are stylistically and rhythmically interlocked with a provided audio drum loop. The proposed system is based on a natural language processing technique: word-based sequence-to-sequence learning using LSTM units. The novelty of the proposed method lies in the fact that the system is not reliant on a voice-by-voice transcription of drums; instead, in this method, a drum representation is used as an input sequence from which a translated bassline is obtained at the output. The drum representation consists of fixed size sequences of onsets detected from a 2-bar audio drum loop in eight different frequency bands. The basslines generated by this method consist of pitched notes with different duration. The proposed system was trained on two distinct datasets compiled for this project by the authors. Each dataset contains a variety of 2-bar drum loops with annotated basslines from two different styles of dance music: House and Soca. A listening experiment designed based on the system revealed that the proposed system is capable of generating basslines that are interesting and are well rhythmically interlocked with the drum loops from which they were generated.},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper040.pdf},
  pdf = {http://www.nime.org/proceedings/2019/nime2019_paper040.pdf},
  code = {https://github.com/behzadhaki/bassline_seq2seq},
  preview={Dalle-drum_and_bass_guitar_interlocking.png}
}
