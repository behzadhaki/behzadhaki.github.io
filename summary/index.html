<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Real-time Drum Accompaniment Generation | Behzad Haki</title> <meta name="author" content="Behzad Haki"/> <meta name="description" content="Summary of my PhD activities so far"/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ü•ÅÔ∏è</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://behzadhaki.github.io/summary/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Behzad¬†</span>Haki</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Real-time Drum Accompaniment Generation</h1> <p class="post-description">Summary of my PhD activities so far</p> </header> <article> <blockquote> <p><strong>SCAN THIS QR CODE TO ACCESS THIS PAGE ON YOUR MOBILE DEVICE</strong> summary_qr <img src="/assets/img/summary_qr.png" alt="dev_stages" style="width: 200px;"> <strong>OR go to <code class="language-plaintext highlighter-rouge">https://behzadhaki.github.io/summary/</code></strong></p> </blockquote> <h2> <br><br><br> </h2> <h2 id="background"><strong>Background</strong></h2> <hr> <p>The main objective of my PhD has been the <strong>development</strong> of a system that can generate <strong>real-time drum accompaniments for live instrumental performances</strong>.</p> <p>The idea here was that instead of focusing mainly on the generative process from an architectural perspective, I wanted to also focus on the interaction between the system and the performer.</p> <p><img src="/assets/img/dev_stages.png" alt="dev_stages" style="width: 700px;"></p> <ul> <li>small models</li> <li>deployed in an easy-to-use environment</li> <li>allowing control over the generation process</li> </ul> <p>In order to be able to use small models, we focus on loop generation, which is a more constrained task than long-term accompaniment generation.</p> <p>Also, instead of analyzing the harmonic content of the input, we focus on the rhythmic content. This will allow us to (1) generate drum accompaniments that are rhythmically interlocked with the input, and (2) given that we didn‚Äôt want to create a human-like system, this constraint will naturally limit the human-like behavior of the system.</p> <h2 id="-1"> <br><br><br> </h2> <h2 id="1---drum-generation-using-transformers"><strong>1 - Drum Generation Using Transformers</strong></h2> <hr> <p>In my first year, we developed an offline long-term drum generation system using Transformer-XL architecture.</p> <div class="publications"> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/thomasTransformerXL.jpg"></div> <div id="NIME21_33" class="col-sm-8"> <div class="title">Transformer Neural Networks for Automated Rhythm Generation</div> <div class="author"> Thomas Nuttall,¬†<em>Behzad Haki</em>,¬†and¬†Sergi Jorda</div> <div class="periodical"> Jun 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://nime.pubpub.org/pub/8947fhly/release/1" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/thomasgnuttall/bumblebeat" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://youtu.be/Ul9s8qSMUgU" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Slides</a> <a href="https://www.nime.org/proc/nime21_33/index.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="abstract hidden"> <p>Recent applications of Transformer neural networks in the field of music have demonstrated their ability to effectively capture and emulate long-term dependencies characteristic of human notions of musicality and creative merit. We propose a novel approach to automated symbolic rhythm generation, where a Transformer-XL model trained on the Magenta Groove MIDI Dataset is used for the tasks of sequence generation and continuation. Hundreds of generations are evaluated using blind-listening tests to determine the extent to which the aspects of rhythm we understand to be valuable are learnt and reproduced. Our model is able to achieve a standard of rhythmic production comparable to human playing across arbitrarily long time periods and multiple playing styles.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">NIME21_33</span><span class="p">,</span>
  <span class="na">article-number</span> <span class="p">=</span> <span class="s">{33}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nuttall, Thomas and Haki, Behzad and Jorda, Sergi}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Transformer Neural Networks for Automated Rhythm Generation}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Conference on New Interfaces for Musical Expression}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Shanghai, China}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2220-4806}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.21428/92fbeb44.fe9a0d82}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://nime.pubpub.org/pub/8947fhly}</span><span class="p">,</span>
  <span class="na">presentation-video</span> <span class="p">=</span> <span class="s">{https://youtu.be/Ul9s8qSMUgU}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> <p>While the system was able to generate ‚Äòrealistic‚Äô long-term drum patterns, it was not suitable for real-time generation due to the computational complexity of the model. Moreover, I wanted to focus on generating drums conditioned on live instrumental performances.</p> <p>As such, we decided to re-think our approach and focus on loop generation using smaller models. Inspired with Google Magenta‚Äôs GrooVAE, we explored whether we can use transformers to generate drum loops. Moreover, in order to speed-up the performance in real-time deployment, we also explored whether we can avoid tokenization of the input and output sequences. That is, whether we could use a very simple representation of our sequences.</p> <p><br></p> <blockquote> <p><strong>How to represent drum loops?</strong></p> <p>Grid relative piano-roll representation. Tempo-agnostic, and focus on 4/4 time signature.</p> <p><img src="/assets/img/aimc2021/hvo.jpeg" alt="hvo" style="width: 700px;"></p> </blockquote> <p><br></p> <blockquote> <p><strong>How to condition on live performances?</strong></p> <p>Use a simple rhythm extracted from the input performance. Then, develop a system that can convert this rhythm into a drum loop.</p> <p><img src="/assets/img/aimc2021/groove2drum.png" alt="dev_stages" style="width: 700px;"></p> </blockquote> <p><br></p> <blockquote> <p><strong>How to extract rhythm from live performances?</strong></p> <p>Simply flatten all the notes into a single track, then represent similar to the drum loop representation.</p> <p><img src="/assets/img/aimc2021/flatten.png" alt="dev_stages" style="width: 350px;"></p> </blockquote> <p><br></p> <blockquote> <p><strong>Final Architecture to Train</strong></p> <p><img src="/assets/img/aimc2021/arch_aimc.jpeg" alt="dev_stages" style="width: 700px;"></p> </blockquote> <p><br></p> <blockquote> <p><strong>Dataset</strong></p> <p>Ideally, we needed paired instrumental performances and drum loops. However, to keep it simple, we initially focused on using a drum only dataset <a href="https://magenta.tensorflow.org/datasets/groove" target="_blank" rel="noopener noreferrer">(Groove MIDI Dataset)</a>. As such, we trained a model that would convert a drum rhythm into a multi-voice drum pattern.</p> <p>In the final real-time system, we could then replace the drum rhythm with the rhythm extracted from the live performance.</p> </blockquote> <blockquote> <p><strong>Read More</strong></p> <p>Details of the methodology and evaluation can be found in the following blog post I‚Äôve prepared:</p> <p><a href="/blog/2022/trainingGrooveTransformer/" class="btn">Blog Post</a></p> </blockquote> <p><br><br><br></p> <hr> <h2 id="2---creating-a-real-time-accompaniment-context-around-the-loop-generator"><strong>2 - Creating a Real-Time Accompaniment Context Around the Loop Generator</strong></h2> <hr> <p><br></p> <blockquote> <p><strong>Main Objective</strong></p> <p>In real-time, extract rhythms from a MIDI performance, convert into a drum loop.</p> <p><img src="/assets/img/aimc2021/real-time-context-camera-ready.png" alt="dev_stages" style="width: 700px;"></p> </blockquote> <p><br></p> <blockquote> <p><strong>Model Training Vs. Inference</strong></p> <p>During training, convert drum groove to drum pattern.</p> <p>In real-time inference, replace drum groove with instrumental groove.</p> <p><img src="/assets/img/aimc2021/pipeline_camera_ready.png" alt="dev_stages" style="width: 700px;"></p> </blockquote> <p><br></p> <blockquote> <p><strong>Deployment</strong></p> <p>Overdub inputs to create a longer evolving accompaniment.</p> <p>A VST using a <a href="https://github.com/pierreguillot/Camomile" target="_blank" rel="noopener noreferrer">Camomile (Pure Data)</a> front-end and a python backend was developed.</p> <p>Pd/Camomile: Visual Interface, Midi Processing, Sequence Playback, and Drum Synthesis</p> <p>Python Backend: Model Inference</p> <p>Communication: OSC</p> <p>In real-time inference, replace drum groove with instrumental groove.</p> <p><img src="/assets/img/aimc2021/real-time-implemented_camera_ready.png" alt="dev_stages" style="width: 700px;"></p> </blockquote> <blockquote> <p><strong>Further details can be found in the following publication:</strong></p> </blockquote> <div class="publications"> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/RealTime_Drum_Accompaniment_Using_Trans_cropped.jpg"></div> <div id="haki_behzad_2022_7088343" class="col-sm-8"> <div class="title">Real-Time Drum Accompaniment Using Transformer Architecture</div> <div class="author"> <em>Behzad Haki</em>,¬†Marina Nieto,¬†Teresa Pelinski, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Sergi Jord√†' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> Sep 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Haki_2022__Real-Time_Drum_Accompaniment_Using_Transformer_Architecture.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/behzadhaki/MonotonicGrooveTransformer" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://youtu.be/XOTksswM_4U" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Slides</a> </div> <div class="abstract hidden"> <p>This paper presents a real-time drum generation system capable of accompanying a human instrumentalist. The drum generation model is a transformer encoder trained to predict a short drum pattern given a reduced rhythmic representation. We demonstrate that with certain design considerations, the short drum pattern generator can be used as a real-time accompaniment in musical sessions lasting much longer than the duration of the training samples. A discussion on the potentials, limitations and possible future continuations of this work is provided.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">haki_behzad_2022_7088343</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Haki, Behzad and Nieto, Marina and Pelinski, Teresa and Jord√†, Sergi}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Real-Time Drum Accompaniment Using Transformer Architecture}}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the 3rd International Conference on on AI and Musical Creativity}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{AIMC}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.5281/zenodo.7088343}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.5281/zenodo.7088343}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> <p><br></p> <blockquote> <p><strong>System Demo</strong></p> <video width="600" height="400" controls=""><source src="/assets/videos/RT_AIMC_trimmed.mp4" type="video/mp4"></source> Your browser does not support the video tag. </video> <p><br></p> <audio width="600" height="200" controls=""><source src="/assets/audio/AIMC_Audio_firstAccomp.mp3" type="audio/mp3"></source>Your browser does not support the video tag.</audio> </blockquote> <p><br><br><br></p> <p><br><br><br></p> <hr> <h2 id="3---improved-deployment-procedure"><strong>3 - Improved Deployment Procedure</strong></h2> <hr> <ul> <li>Pd/Python setup is very cumbersome.</li> <li>Required some technical knowledge to set up the system.</li> <li>To make it more accessible, we had to develop a standalone system.</li> <li>The challenge was that there was very little resources available to develop a standalone system.</li> <li>A good deal of a year was spent on porting the system to a standalone system.</li> <li>This was done using the <a href="https://juce.com/" target="_blank" rel="noopener noreferrer">JUCE</a> framework as well as the <a href="https://pytorch.org/tutorials/advanced/cpp_export.html" target="_blank" rel="noopener noreferrer">TorchScript</a> library.</li> </ul> <blockquote> <p><strong>Standalone Groove2Drum Accompaniment System</strong></p> <video width="600" height="400" controls=""><source src="/assets/videos/Groove2Drum.mp4" type="video/mp4"></source> Your browser does not support the video tag. </video> <p><br></p> </blockquote> <p><a href="https://github.com/behzadhaki/Groove2DrumVST" class="btn" target="_blank" rel="noopener noreferrer">Source Code</a></p> <p>The development of the standalone system was a significant challenge. Once done, I realized that I won‚Äôt be able to develop a standalone system for every upcoming project if I have to go through the same process again and again.</p> <p>As such, I decided to develop a framework that would allow me to easily deploy my models in a standalone system. This framework is called <a href="https://neuralmidifx.github.io/" target="_blank" rel="noopener noreferrer">NeuralMidiFx</a>. The idea behind this framework is to allow researchers to easily deploy their models in a standalone system without having to worry about the technical details of the deployment process.</p> <p>Using the framework, one can very quickly design a graphical interface using a simple JSON file. Then, all the communication between the interface and all the dedicated background threads in handled by the framework.</p> <p>While the development of the framework was extremely costly in terms of time, it has simplified the process to the extent that I can now develop a standalone system in a matter of days. Moreover, the framework is open-source, and I hope that it will be useful for other researchers as well.</p> <p><br></p> <blockquote> <p><strong>Main Idea Behind NeuralMidiFx: Division of Responsibilities</strong></p> <p><img src="/assets/img/nmfx/NMFX_responsibilities.png" alt="dev_stages" style="width: 700px;"></p> </blockquote> <p><br></p> <blockquote> <p><strong>NeuralMidiFx Architecture</strong></p> <p><img src="/assets/img/nmfx/NMFX_Arch.png" alt="dev_stages" style="width: 700px;"></p> </blockquote> <p><br></p> <blockquote> <p><strong>Further details can be found in the following publication:</strong></p> </blockquote> <div class="publications"> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/NMFX.jpg"></div> <div id="Haki2023NeuralMidiFx" class="col-sm-8"> <div class="title">NeuralMidiFx: A Wrapper Template for Deploying Neural Networks as VST3 Plugins</div> <div class="author"> <em>Behzad Haki</em>,¬†Julian Lenz,¬†and¬†Sergi Jorda</div> <div class="periodical"> Sep 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/NeuralMidiFxPaper.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://neuralmidifx.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://aimc2023.pubpub.org/pub/givwzz98" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Slides</a> </div> <div class="abstract hidden"> <p>Proper research, development and evaluation of AI-based generative systems of music that focus on performance or composition require active user-system interactions. To include a diverse group of users that can properly engage with a given system, researchers should provide easy access to their developed systems. Given that many users (i.e. musicians) are non-technical to the field of AI and the development frameworks involved, the researchers should aim to make their systems accessible within the environments commonly used in production/composition workflows (e.g. in the form of plugins hosted in digital audio workstations). Unfortunately, deploying generative systems in this manner is highly expensive. As such, researchers with limited resources are often unable to provide easy access to their works, and subsequently, are not able to properly evaluate and encourage active engagement with their systems. Facing these limitations, we have been working on a solution that allows for easy, effective and accessible deployment of generative systems. To this end, we propose a wrapper/template called NeuralMidiFx, which streamlines the deployment of neural network based symbolic music generation systems as VST3 plugins. The proposed wrapper is intended to allow researchers to develop plugins with ease while requiring minimal familiarity with plugin development. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Haki2023NeuralMidiFx</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Haki, Behzad and Lenz, Julian and Jorda, Sergi}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the 4th International Conference on on AI and Musical Creativity}}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{NeuralMidiFx: A Wrapper Template for Deploying Neural Networks as VST3 Plugins}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> <p><br><br><br></p> <hr> <h2 id="4---in-situ-evaluation-of-the-groove2drum-vst"><strong>4 - In-Situ Evaluation of the Groove2Drum VST</strong></h2> <hr> <ul> <li>The evaluation of the Groove2Drum VST was done in-situ.</li> <li>We asked <a href="https://raulrefree.com/" target="_blank" rel="noopener noreferrer">Raul Refree</a> to come to the lab and test the system.</li> <li>Raul was asked to perform on a MIDI keyboard, and the system would generate drum accompaniments in real-time.</li> <li>Subsequently, we asked him to provide feedback on the system.</li> </ul> <blockquote> <p><strong>Feedback</strong></p> <ul> <li>System was hard to use at first.</li> <li>There are many controls that are overwhelming in a live performance setting.</li> <li>Perhaps the system was more useful in a studio/production setting.</li> <li>If designed for live performance, it should be as autonomous as possible.</li> <li>That said, having some quick controls to change/guide the generation process would be useful.</li> </ul> </blockquote> <p>Following the feedback, we decided to develop a new system that would be more autonomous and would require less user input.</p> <p>To do so, we modified our architecture into a Variational Autoencoder (VAE). This would allow us to generate drum loops in a more controlled manner. That is, we could be able to guide the generation to certain predefined patterns.</p> <p>This was the main focus of the <code class="language-plaintext highlighter-rouge">GrooveTransformer</code> system, discussed in the following section.</p> <p><br><br><br></p> <hr> <h2 id="5---groovetransformer"><strong>5 - GrooveTransformer</strong></h2> <hr> <ul> <li>The main idea behind the <code class="language-plaintext highlighter-rouge">GrooveTransformer</code> system was to develop a system that would allow us to generate drum loops in a more controlled manner.</li> <li>i.e. allow user to fall-back to predefined patterns.</li> <li>Moreover, allow user to decide how much the generations should be guided by the input performance.</li> </ul> <p><br></p> <blockquote> <p><strong>Concept</strong> If we develop a VAE model, then we could achieve the above objectives as follows:</p> <p><img src="/assets/img/GT/latent.png" alt="dev_stages" style="width: 700px;"></p> </blockquote> <p><br></p> <blockquote> <p><strong>Architecture</strong></p> <p>same as before, except the variational layer.</p> <p><img src="/assets/img/GT/ARCH.PNG" alt="dev_stages" style="width: 200px;"></p> </blockquote> <p><br></p> <blockquote> <p><strong>Deployment</strong></p> <p>The system was deployed in a standalone system using the NeuralMidiFx framework.</p> <video width="600" height="400" controls=""><source src="/assets/videos/plugin.webm" type="video/webm"></source> Your browser does not support the video tag. </video> </blockquote> <p><br></p> <blockquote> <p><strong>Demos</strong></p> <video width="600" height="400" controls=""><source src="/assets/videos/VCV_VST_Keyboard_LowRes.mp4" type="video/mp4"></source> Your browser does not support the video tag. </video> <p><br> <a href="https://generativedata.github.io/generated_examples/jam_session_recordings/" class="btn" target="_blank" rel="noopener noreferrer">More Recordings Here</a></p> </blockquote> <p><br></p> <blockquote> <p><strong>Refree‚Äôs Live Rehearsal at CCCB</strong></p> <video width="600" height="400" controls=""><source src="/assets/videos/arturia.mp4" type="video/mp4"></source> Your browser does not support the video tag. </video> <p><br></p> <video width="600" height="400" controls=""><source src="/assets/videos/phone.mp4" type="video/mp4"></source> Your browser does not support the video tag. </video> </blockquote> <p><br><br><br></p> <hr> <h2 id="6---groovetransformer-in-euro-rack-format"><strong>6 - GrooveTransformer in Euro-rack Format</strong></h2> <hr> <ul> <li>Main objective was to push the users to interact with the system in a context that the model was not developed for.</li> <li>i.e. use unconventional rhythms and also synthesize the drums in a more unconventional manner.</li> </ul> <p><br></p> <blockquote> <p><strong>Final Prototype</strong></p> <p><img src="/assets/img/GT/GT_Euro_.jpg" alt="dev_stages" style="width: 700px;"></p> </blockquote> <p><br></p> <blockquote> <p><strong>Deployment Procedure</strong></p> <p><img src="/assets/img/GT/hardware.png" alt="dev_stages" style="width: 700px;"></p> </blockquote> <p><br></p> <blockquote> <p><strong>Demos</strong></p> <p>Setup:</p> <p>For the GrooveTransformer‚Äôs input groove, it receives a multiple of the Intellijel Metropolix gate and pitch sequence that controls the Acid Technology Chainsaw voice in the Eurorack. <strong>In this scenario, we have opted to use the pitch values of each note event to represent the velocity of the input</strong>. The pitch and gate voltages are sent from the Eurorack to the GrooveTransformer via an Expert Sleepers ES-9 DC-coupled audio interface and a Cardinal CV to MIDI converter plug-in. Generated drum patterns from the GrooveTransformer are converted to Control Voltage (CV) and sent to the Eurorack via a PolyEnd Poly2 MIDI to CV converter.</p> <p>Drum Synthesis:</p> <p>We use 7 voices of the generated patterns (kick, snare, open and closed hi-hat, and lo/mid/hi tom) to trigger 4 voices in the Eurorack: Kick: Schlappi Engineering Angle Grinder + Make Noise Moddemix VCA + Intellijel Quadrax envelope generator Snare: Intellijel Plonk Open and Closed Hi-Hats: Basimilus Iteritas Alter Lo, Mid, Hi Toms: Akemie‚Äôs Taiko</p> <p>To retain generated dynamics, the kick, hi-hats, and toms are routed to individual channels on a Mutable Instruments Veils. The level of each channel is controlled with the velocity sequence associated with the corresponding voice The Intellijel Plonk has a dedicated velocity input that we utilized rather than routing the signal to Veils</p> <video width="600" height="400" controls=""><source src="/assets/videos/eurorackJam.mp4" type="video/mp4"></source> Your browser does not support the video tag. </video> </blockquote> <p><br></p> <blockquote> <p><strong>Further details can be found in the following publication:</strong></p> </blockquote> <div class="publications"> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/GT_Euro_.jpg"></div> <div id="Haki2024GrooveTransformer" class="col-sm-8"> <div class="title">GrooveTransformer: A Generative Drum Sequencer Eurorack Module</div> <div class="author"> Nicholas Evans,¬†<em>Behzad Haki</em>,¬†and¬†Sergi Jorda</div> <div class="periodical"> Sep 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/NIME_2024_GrooveTransformer.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://groovetransformer.github.io" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/assets/pdf/" class="btn btn-sm z-depth-0" role="button">Slides</a> <a href="https://groovetransformer.github.io" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="abstract hidden"> <p>This paper presents the GrooveTransformer, a Eurorack module designed for generative drum sequencing. Central to its design is a Variational Auto-Encoder (VAE), around which we have designed a deployment context enabling performance through accompaniment and/or user interaction. This module allows the user to use the system as an accompaniment generator while interacting with the generative processes in real-time. In this paper, we review the design principles and technical architecture of the module, while also discussing the potentials and short-comings of our work.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Haki2024GrooveTransformer</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Evans, Nicholas and Haki, Behzad and Jorda, Sergi}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Conference on New Interfaces for Musical Expression (NIME) 2024}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{NIME}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{GrooveTransformer: A Generative Drum Sequencer Eurorack Module}}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> <p><br><br><br></p> <hr> <h2 id="6---improved-controllability"><strong>6 - Improved Controllability</strong></h2> <hr> <ul> <li>Using user feedback, we decided to improve the controllability of the system.</li> <li>Added Genre control and Voice Redistribution controls:</li> </ul> <p><br></p> <blockquote> <p><strong>Architecture</strong></p> <p><img src="/assets/img/control_GT/control_models.png" alt="dev_stages" style="width: 700px;"></p> </blockquote> <p><br></p> <blockquote> <p><strong>Deployment</strong></p> <p><a href="https://generativedata.github.io/resources/source_code_and_vst_plugins/" class="btn" target="_blank" rel="noopener noreferrer">Download the VST Plugin</a></p> </blockquote> <p><br></p> <blockquote> <p><strong>Note</strong></p> <p>Currently, under review for ISMIR 2024. However, extra information can be found <a href="https://generativedata.github.io/" class="btn" target="_blank" rel="noopener noreferrer">here</a></p> </blockquote> <h2 id="-2"> <br><br><br> </h2> <h2 id="7---adaptation-to-audio"><strong>7 - Adaptation to Audio</strong></h2> <hr> <ul> <li>The main objective was to adapt the system to audio input.</li> <li>We had done one experiment before, which used audio input with the same architecture employed for the symbolic2symbolic systems.</li> <li>Read the following publication for more details:</li> </ul> <div class="publications"> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/Completing.JPG"></div> <div id="Haki2023Completing" class="col-sm-8"> <div class="title">Completing Audio Drum Loops with Symbolic Drum Suggestions</div> <div class="author"> <em>Behzad Haki</em>,¬†Teresa Pelinski,¬†Marina Nieto, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Sergi Jorda' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> Apr 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/CompletingAudioLoops.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://transformergrooveinfilling.github.io" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/assets/pdf/" class="btn btn-sm z-depth-0" role="button">Slides</a> <a href="https://www.nime.org/proc/nime2023_34/index.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="abstract hidden"> <p>Sampled drums can be used as an affordable way of creating human-like drum tracks, or perhaps more interestingly, can be used as a mean of experimentation with rhythm and groove. Similarly, AI-based drum generation tools can focus on creating human-like drum patterns, or alternatively, focus on providing producers/musicians with means of experimentation with rhythm. In this work, we aimed to explore the latter approach. To this end, we present a suite of Transformer-based models aimed at completing audio drum loops with stylistically consistent symbolic drum events. Our proposed models rely on a reduced spectral representation of the drum loop, striking a balance between a raw audio recording and an exact symbolic transcription. Using a number of objective evaluations, we explore the validity of our approach and identify several challenges that need to be further studied in future iterations of this work. Lastly, we provide a real-time VST plugin that allows musicians/producers to utilize the models in real-time production settings.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Haki2023Completing</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Haki, Behzad and Pelinski, Teresa and Nieto, Marina and Jorda, Sergi}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Conference on New Interfaces for Musical Expression (NIME) 2023}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{NIME}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Completing Audio Drum Loops with Symbolic Drum Suggestions}}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> <blockquote> <p><strong>Previous Work on Audio Input</strong></p> <p><img src="/assets/img/audio2drum/pipeline_1.5x.png" alt="dev_stages" style="width: 700px;"></p> </blockquote> <p><br></p> <ul> <li>The initial idea was to use the above approach for adapting the current systems to audio input.</li> <li>Given time constraints, we decided to use a simpler approach.</li> <li>We use a publicly available onset detection system called <a href="https://github.com/chrisdonahue/ddc_onset" target="_blank" rel="noopener noreferrer">DDC_Onset</a> to extract onsets from the audio input.</li> <li>We then convert these onsets into a drum loop using the GrooveTransformer system.</li> <li>The system was deployed in a standalone system using the NeuralMidiFx framework.</li> </ul> <blockquote> <p><strong>Deployment</strong> Developed plugins will be released soon.</p> </blockquote> <p><br></p> <blockquote> <p><strong>Demos</strong></p> <video width="600" height="400" controls=""><source src="/assets/videos/AudioInputDemo1.mp4" type="video/mp4"></source> Your browser does not support the video tag. </video> </blockquote> <h2 id="-3"> <br><br><br> </h2> <h2 id="8---going-beyond-drum-grooves"><strong>8 - Going Beyond Drum Grooves</strong></h2> <hr> <ul> <li>Objective here was to develop the system such that it had knowledge of the type of instrument being played.</li> <li>i.e. It was able to model the rhythmic interplay between different instruments and drums.</li> <li>For this, we developed a number of Groove2Groove conversion systems using the same architecture as the GrooveTransformer system.</li> <li>These systems are then paired with the GrooveTransformer system to generate drum loops that are interlocked with the input performance.</li> <li>for the data, we‚Äôve used the <a href="https://colinraffel.com/projects/lmd/" target="_blank" rel="noopener noreferrer">Lakh MIDI Dataset</a>. In here, there are many multi-track midis that we can use to train the Groove2Groove systems.</li> <li>For these systems, we‚Äôve focused on Bass-Drum, Guitar-Drum, and Piano-Drum pairs.</li> </ul> <p><br></p> <ul> <li>The details of this work will be released soon.</li> </ul> <blockquote> <p><strong>Demos</strong></p> <audio width="600" height="200" controls=""><source src="/assets/audio/G2GTest2_June22Edit.mp3" type="audio/mp3"></source>Your browser does not support the video tag.</audio> </blockquote> <h2 id="-4"> <br><br><br> </h2> <h2 id="9---final-experiment-on-going-long-term-accompaniment-generation"><strong>9 - Final Experiment (On-going): Long-term Accompaniment Generation</strong></h2> <hr> <ul> <li>The main objective here is to develop a system that can generate long-term accompaniments without overdubbing</li> <li>That is, we want the system to consider the entirety of the performance prior to generate the accompaniment.</li> </ul> <p><img src="/assets/img/LongPerformanceArch.png" alt="dev_stages" style="width: 700px;"></p> </article> </div> </div> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>